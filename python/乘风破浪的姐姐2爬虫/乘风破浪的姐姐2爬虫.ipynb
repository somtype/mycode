{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "def crawl_wiki_data2():\n",
    "    \"\"\"\n",
    "    爬取百度百科中《乘风破浪的姐姐》中嘉宾信息，返回html\n",
    "    \"\"\"\n",
    "    headers = { \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'\n",
    "    }\n",
    "    url='https://baike.baidu.com/item/乘风破浪的姐姐'                         \n",
    "\n",
    "    try:\n",
    "        response = requests.get(url,headers=headers)\n",
    "        #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串\n",
    "        soup = BeautifulSoup(response.text,'lxml')     \n",
    "          \n",
    "        #返回所有的<table>所有标签\n",
    "        tables = soup.find_all('table')\n",
    "        #print(tables)\n",
    "        \n",
    "        crawl_table_title = \"按姓氏首字母排序\"\n",
    "        table_attress=[];\n",
    "        for table in  tables:           \n",
    "            #对当前节点前面的标签和字符串进行查找\n",
    "            table_titles = table.find_previous('div')\n",
    "            #print (1,table_titles)\n",
    "            for title in table_titles:\n",
    "                if(crawl_table_title in title):\n",
    "                    #print (table)\n",
    "                    table_attress.append(table)\n",
    "                    #return table       \n",
    "        return table_attress\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_wiki_data(table_html):\n",
    "    '''\n",
    "    解析得到选手信息，包括包括选手姓名和选手个人百度百科页面链接，存JSON文件,保存到work目录下\n",
    "    '''\n",
    "    bs = BeautifulSoup(str(table_html),'lxml')\n",
    "    all_trs = bs.find_all('tr')\n",
    "\n",
    "    stars = []\n",
    "    for tr in all_trs:\n",
    "         all_tds = tr.find_all('td')   #tr下面所有的td          \n",
    "\n",
    "         for td in  all_tds:\n",
    "             #star存储选手信息，包括选手姓名和选手个人百度百科页面链接\n",
    "             star = {}    \n",
    "             if td.find('a'):\n",
    "                 #找选手名称和选手百度百科连接\n",
    "                 if td.find_next('a'):\n",
    "                    star[\"name\"]=td.find_next('a').text\n",
    "                    #print(len(td.find_next('a').text),td.find_next('a').text)\n",
    "                    star['link'] =  'https://baike.baidu.com' + td.find_next('a').get('href')\n",
    "\n",
    "                 elif td.find_next('div'):\n",
    "                     #print(len(td.find_next('div').find('a').text),td.find_next('div').find('a').text)\n",
    "                     star[\"name\"]=td.find_next('div').find('a').text\n",
    "                     star['link'] = 'https://baike.baidu.com' + td.find_next('div').find('a').get('href')\n",
    "                 #print(len(star[\"name\"]),star[\"name\"])\n",
    "                 if (star[\"name\"]!=\" \"):\n",
    "                    stars.append(star)\n",
    "      \n",
    "    json_data = json.loads(str(stars).replace(\"\\'\",\"\\\"\"))   \n",
    "    with open('work/' + 'stars.json', 'w', encoding='UTF-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawl_everyone_wiki_urls():\n",
    "    '''\n",
    "    爬取每个选手的百度百科图片，并保存\n",
    "    ''' \n",
    "    with open('work/' + 'stars.json', 'r', encoding='UTF-8') as file:\n",
    "         json_array = json.loads(file.read())\n",
    "    headers = { \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' \n",
    "     }  \n",
    "    star_infos = []\n",
    "    for star in json_array:\n",
    "        star_info = {}       \n",
    "        name = star['name']\n",
    "        if (ord(name[0])==10):\n",
    "             continue\n",
    "        link = star['link']\n",
    "        star_info['name'] = name\n",
    "        #向选手个人百度百科发送一个http get请求\n",
    "        response = requests.get(link,headers=headers)        \n",
    "        #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象\n",
    "        bs = BeautifulSoup(response.text,'lxml')       \n",
    "        #获取选手的民族、星座、血型、体重等信息\n",
    "        base_info_div = bs.find('div',{'class':'basic-info cmn-clearfix'})\n",
    "        dls = base_info_div.find_all('dl')\n",
    "        \n",
    "        for dl in dls:\n",
    "            dts = dl.find_all('dt')\n",
    "            for dt in dts:\n",
    "                if \"\".join(str(dt.text).split()) == '民族':\n",
    "                     star_info['nation'] = dt.find_next('dd').text\n",
    "                if \"\".join(str(dt.text).split()) == '星座':\n",
    "                     star_info['constellation'] = dt.find_next('dd').text\n",
    "                if \"\".join(str(dt.text).split()) == '血型':  \n",
    "                     star_info['blood_type'] = dt.find_next('dd').text\n",
    "                if \"\".join(str(dt.text).split()) == '身高':  \n",
    "                     height_str = str(dt.find_next('dd').text)\n",
    "                     star_info['height'] = str(height_str[0:height_str.rfind('cm')]).replace(\"\\n\",\"\")\n",
    "                if \"\".join(str(dt.text).split()) == '体重':  \n",
    "                     star_info['weight'] = str(dt.find_next('dd').text).replace(\"\\n\",\"\")\n",
    "                if \"\".join(str(dt.text).split()) == '出生日期':  \n",
    "                     birth_day_str = str(dt.find_next('dd').text).replace(\"\\n\",\"\")\n",
    "                     if '年' in  birth_day_str:\n",
    "                         star_info['birth_day'] = birth_day_str[0:birth_day_str.rfind('年')]\n",
    "        star_infos.append(star_info) \n",
    "        \n",
    "        #从个人百度百科页面中解析得到一个链接，该链接指向选手图片列表页面\n",
    "        if bs.select('.summary-pic a'):\n",
    "               pic_list_url = bs.select('.summary-pic a')[0].get('href')\n",
    "               pic_list_url = 'https://baike.baidu.com' + pic_list_url\n",
    "        \n",
    "               #向选手图片列表页面发送http get请求\n",
    "               pic_list_response = requests.get(pic_list_url,headers=headers)\n",
    "\n",
    "               #对选手图片列表页面进行解析，获取所有图片链接\n",
    "               bs = BeautifulSoup(pic_list_response.text,'lxml')\n",
    "               pic_list_html=bs.select('.pic-list img ')\n",
    "               pic_urls = []\n",
    "               for pic_html in pic_list_html: \n",
    "                    pic_url = pic_html.get('src')\n",
    "                    pic_urls.append(pic_url)\n",
    "                    #根据图片链接列表pic_urls, 下载所有图片，保存在以name命名的文件夹中\n",
    "                    down_save_pic(name,pic_urls)\n",
    "        #将个人信息存储到json文件中\n",
    "        \n",
    "        json_data = json.loads(str(star_infos).replace(\"\\'\",\"\\\"\").replace(\"\\\\xa0\",\"\")) \n",
    "        with open('work/' + 'stars_info.json', 'w', encoding='UTF-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def down_save_pic(name,pic_urls):\n",
    "    '''\n",
    "    根据图片链接列表pic_urls, 下载所有图片，保存在以name命名的文件夹中,\n",
    "    '''\n",
    "    path = 'work/'+'pics/'+name+'/'\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "\n",
    "    for i, pic_url in enumerate(pic_urls):\n",
    "        try:\n",
    "            pic = requests.get(pic_url, timeout=15)\n",
    "            string = str(i + 1) + '.jpg'\n",
    "            with open(path+string, 'wb') as f:\n",
    "                f.write(pic.content)\n",
    "                #print('成功下载第%s张图片: %s' % (str(i + 1), str(pic_url)))\n",
    "        except Exception as e:\n",
    "            #print('下载第%s张图片时失败: %s' % (str(i + 1), str(pic_url)))\n",
    "            print(e)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "     #爬取百度百科中《乘风破浪的姐姐》中参赛选手信息，返回html\n",
    "     htmls = crawl_wiki_data2()\n",
    "\n",
    "     #解析html,得到第一季选手信息，保存为json文件\n",
    "     parse_wiki_data(htmls[1])\n",
    "     #从每个选手的百度百科页面上爬取,并保存\n",
    "     crawl_everyone_wiki_urls()\n",
    "\n",
    "     print(\"所有信息爬取完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import json\n",
    "import matplotlib.font_manager as font_manager\n",
    "#显示matplotlib生成的图形\n",
    "# %matplotlib inline\n",
    "\n",
    "with open('work/stars_info.json', 'r', encoding='UTF-8') as file:\n",
    "         json_array = json.loads(file.read())`  \n",
    "\n",
    "#绘制选手年龄分布柱状图,x轴为年龄，y轴为该年龄的小姐姐数量\n",
    "birth_days = []\n",
    "for star in json_array:\n",
    "    if 'birth_day' in dict(star).keys():\n",
    "        birth_day = star['birth_day'] \n",
    "        if len(birth_day) == 4:  \n",
    "            birth_days.append(birth_day)\n",
    "\n",
    "birth_days.sort()\n",
    "print(birth_days)\n",
    "\n",
    "birth_days_list = []\n",
    "count_list = []\n",
    "for birth_day in birth_days:\n",
    "    if birth_day not in birth_days_list:\n",
    "        count = birth_days.count(birth_day)\n",
    "        birth_days_list.append(birth_day)\n",
    "        count_list.append(count)\n",
    "\n",
    "print(birth_days_list)\n",
    "print(count_list)\n",
    "\n",
    "# 设置显示中文\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(range(len(count_list)), count_list,color='r',tick_label=birth_days_list,\n",
    "            facecolor='#9999ff',edgecolor='white')\n",
    "\n",
    "# 这里是调节横坐标的倾斜度，rotation是度数，以及设置刻度字体大小\n",
    "plt.xticks(rotation=45,fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('''《乘风破浪的姐姐》参赛嘉宾''',fontsize = 24)\n",
    "plt.savefig('work/result/bar_result01.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "#显示matplotlib生成的图形\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_json('work/stars_info.json',dtype = {'birth_day' : str})\n",
    "#print(df)\n",
    "df = df[df['birth_day'].map(len) == 4]\n",
    "#print(df)\n",
    "\n",
    "grouped=df['name'].groupby(df['birth_day'])\n",
    "#print(grouped)\n",
    "s = grouped.count()\n",
    "birth_days_list = s.index\n",
    "count_list = s.values\n",
    "\n",
    "# 设置显示中文\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(range(len(count_list)), count_list,color='r',tick_label=birth_days_list,\n",
    "        facecolor='#9999ff',edgecolor='white')\n",
    "# 这里是调节横坐标的倾斜度，rotation是度数，以及设置刻度字体大小\n",
    "plt.xticks(rotation=45,fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend()\n",
    "plt.title('''《乘风破浪的姐姐》参赛嘉宾''',fontsize = 24)\n",
    "plt.savefig('work/result/bar_result02.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import json\n",
    "import matplotlib.font_manager as font_manager\n",
    "#显示matplotlib生成的图形\n",
    "%matplotlib inline\n",
    "\n",
    "with open('work/stars_info.json', 'r', encoding='UTF-8') as file:\n",
    "         json_array = json.loads(file.read())\n",
    "\n",
    "#绘制选手体重分布饼状图\n",
    "weights = []\n",
    "counts = []\n",
    "\n",
    "for star in json_array:\n",
    "    if 'weight' in dict(star).keys():\n",
    "        weight = float(star['weight'][0:2])\n",
    "        weights.append(weight)\n",
    "print(weights)\n",
    "\n",
    "size_list = []\n",
    "count_list = []\n",
    "\n",
    "size1 = 0\n",
    "size2 = 0\n",
    "size3 = 0\n",
    "size4 = 0\n",
    "\n",
    "for weight in weights:\n",
    "    if weight <=45:\n",
    "        size1 += 1\n",
    "    elif 45 < weight <= 50:\n",
    "        size2 += 1\n",
    "    elif 50 < weight <= 55:\n",
    "        size3 += 1\n",
    "    else:\n",
    "        size4 += 1\n",
    "\n",
    "labels = '<=45kg', '45~50kg', '50~55kg', '>55kg'\n",
    "\n",
    "sizes = [size1, size2, size3, size4]\n",
    "explode = (0.2, 0.1, 0, 0)  \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True)\n",
    "ax1.axis('equal') \n",
    "plt.savefig('work/result/pie_result01.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import json\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "#显示matplotlib生成的图形\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_json('work/stars_info.json')\n",
    "print(df)\n",
    "weights=df['weight']\n",
    "arrs = weights.values\n",
    "\n",
    "arrs = [x for x in arrs if not pd.isnull(x)]\n",
    "for i in range(len(arrs)):   \n",
    "    arrs[i] = float(arrs[i][0:2])\n",
    "\n",
    "#pandas.cut用来把一组数据分割成离散的区间。比如有一组年龄数据，可以使用pandas.cut将年龄数据分割成不同的年龄段并打上标签。bins是被切割后的区间.\n",
    "bin=[0,45,50,55,100]\n",
    "se1=pd.cut(arrs,bin)\n",
    "print(se1)\n",
    "\n",
    "#pandas的value_counts()函数可以对Series里面的每个值进行计数并且排序。\n",
    "pd.value_counts(se1)\n",
    "\n",
    "sizes = pd.value_counts(se1)\n",
    "print(sizes)\n",
    "labels = '45~50kg', '<=45kg','50~55kg', '>55kg'\n",
    "explode = (0.2, 0.1, 0, 0)  \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal') \n",
    "plt.savefig('work/result/pie_result02.jpg') \n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}